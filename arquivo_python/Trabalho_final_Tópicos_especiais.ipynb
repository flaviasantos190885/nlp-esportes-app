{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP3fwqR7inGxiuZbK8ddAS9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"fd7b5b2f25d547f38ca1391c4fc24265":{"model_module":"@jupyter-widgets/controls","model_name":"DropdownModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DropdownModel","_options_labels":["(Nenhum)","raw_debug.txt"],"_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"DropdownView","description":"Exemplo:","description_tooltip":null,"disabled":false,"index":0,"layout":"IPY_MODEL_20ef53dc2a194f08aab8debe9ba02379","style":"IPY_MODEL_d5ccd487d53c42f992d357ff507a5955"}},"20ef53dc2a194f08aab8debe9ba02379":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"80%"}},"d5ccd487d53c42f992d357ff507a5955":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"85ec04e5f05a453f9c367758068eabfd":{"model_module":"@jupyter-widgets/controls","model_name":"TextareaModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"TextareaModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"TextareaView","continuous_update":true,"description":"Entrada:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_557505e3354e4542844331e640915264","placeholder":"Digite ou cole o texto aqui (tema: esportes)","rows":null,"style":"IPY_MODEL_01c3ef97dcc740348c86b9820cf00737","value":"Flamengo"}},"557505e3354e4542844331e640915264":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":"160px","justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"98%"}},"01c3ef97dcc740348c86b9820cf00737":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"042c07c99e3c4c2ea72dcfb0619a9ec8":{"model_module":"@jupyter-widgets/controls","model_name":"DropdownModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DropdownModel","_options_labels":["Gerar texto","Resumir","Traduzir PT->EN","Traduzir EN->PT","Pergunta/Resposta"],"_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"DropdownView","description":"Tarefa:","description_tooltip":null,"disabled":false,"index":0,"layout":"IPY_MODEL_37ae4d0a411047aeafa9826245100f20","style":"IPY_MODEL_e6faef02f4494d1c895b9451d2fe3a2f"}},"37ae4d0a411047aeafa9826245100f20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6faef02f4494d1c895b9451d2fe3a2f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e0bd5d3997e4ea6aa12f960f0db17fa":{"model_module":"@jupyter-widgets/controls","model_name":"TextModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"TextModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"TextView","continuous_update":true,"description":"Modelo (opcional):","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_6af8a25a9e8d42aca14c2d4f17753fef","placeholder":"â€‹","style":"IPY_MODEL_d4cce9a577024e10b57c5e61294b1d3d","value":""}},"6af8a25a9e8d42aca14c2d4f17753fef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"80%"}},"d4cce9a577024e10b57c5e61294b1d3d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a0327611feb4d38865bbd181a229d1e":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"success","description":"Executar","disabled":false,"icon":"","layout":"IPY_MODEL_136f39e9b4b045f7b5fd3089ac01c9a0","style":"IPY_MODEL_9f2106118ea44adaa24a87dd0934cb57","tooltip":""}},"136f39e9b4b045f7b5fd3089ac01c9a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f2106118ea44adaa24a87dd0934cb57":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"772f3b6200a1426297da0ca6efe5d664":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_499e477dcbe648d695d756feb7e16506","msg_id":"","outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ” Buscando resumo sobre 'Flamengo' na Wikipedia (PT)...\n"]},{"output_type":"stream","name":"stdout","text":["\n","ğŸŸ¢ Resultado (Wikipedia):\n"," Clube de Regatas do Flamengo (mais conhecido simplesmente como Flamengo, popularmente pelos apelidos de Fla, Mengo e MengÃ£o, e cujo acrÃ´nimo Ã© CRF) Ã© uma agremiaÃ§Ã£o poliesportiva brasileira com sede na cidade do Rio de Janeiro, capital do estado homÃ´nimo. Fundado no bairro do Flamengo para disputas do esporte remo em 17 de novembro de 1895, tornou-se um dos clubes mais bem-sucedidos e populares do esporte brasileiro especialmente pelo futebol. Ã‰ considerado um dos maiores e mais tradicionais clubes do Brasil e da AmÃ©rica do Sul. Tem como suas cores tradicionais o vermelho e o preto e como seus maiores rivais esportivos o Vasco da Gama, o Fluminense e o Botafogo.\n","\n","Dentre seus maiores tÃ­tulos no futebol, destacam-se as conquistas da Copa Intercontinental (Ãºnico time carioca a ter conquistado um tÃ­tulo de dimensÃ£o mundial reconhecido pela FIFA) e das Copas Libertadores da AmÃ©rica de 1981, 2019 e 2022 (Ãºnico time carioca a ter conquistado por trÃªs vezes a competiÃ§Ã£o, e um dos sete do Brasil a tÃª-la conquistado mais de uma vez), alÃ©m de uma Recopa Sul-Americana, uma Copa Mercosul e uma Copa de Ouro NicolÃ¡s Leoz, o que lhe confere a quarta posiÃ§Ã£o no ranking de tÃ­tulos internacionais de clubes brasileiros. Em se tratando de Copa Libertadores da AmÃ©rica o Flamengo Ã© o quarto com maior aproveitamento na competiÃ§Ã£o, alÃ©m de ser o clube com o melhor desempenho considerando apenas duelos entre equipes brasileiras atÃ© 2019. Em relaÃ§Ã£o Ã s conquistas nacionais o Flamengo Ã©, por decisÃ£o judicial, e em seguida, pela CBF, oficialmente detentor de sete tÃ­tulos do Campeonato Brasileiro (1980, 1982, 1983, 1992, 2009, 2019 e 2020), conquistou tambÃ©m a Copa UniÃ£o (TrofÃ©u JoÃ£o Havelange) â€“ com reconhecimento limitado pela CBF â€“, cinco tÃ­tulos da Copa do Brasil, trÃªs Supercopas do Brasil e uma Copa dos CampeÃµes. Estas conquistas dÃ£o ao clube o segundo lugar no ranking de tÃ­tulos nacionais, atrÃ¡s apenas do Palmeiras (18 conquistas). Ã‰ tambÃ©m, segundo um levantamento feito pela ESPN Brasil, o primeiro clube do Brasil a ter conquistado todos os tÃ­tulos nacionais e internacionais possÃ­veis.\n","\n","O Flamengo Ã© o clube de futebol mais popular do Brasil, com uma torcida estimada em 40,4 milhÃµes de torcedores espalhados por todas as regiÃµes do Brasil. Segundo levantamento conduzido pela agÃªncia de marketing desportivo Gerardo Molina-Euroamerica, o Flamengo Ã©, em nÃºmeros absolutos, o clube de futebol com o maior nÃºmero de seguidores em todo o mundo. Em razÃ£o da forÃ§a de sua torcida, Ã© o clube brasileiro que mais recebe valores de direitos de transmissÃ£o. Desde 2018, o Flamengo Ã© considerado o clube mais valioso do Brasil, tornando-se em 2019, o time mais valioso da AmÃ©rica do Sul, alÃ©m de ser o 70Âº time de futebol mais valioso do mundo, avaliado em mais de 145,7 milhÃµes de euros. Um Fla-Flu detÃ©m o recorde mundial de pÃºblico de partidas entre clubes: 194 603 espectadores, na final do Campeonato Carioca de 1963, vencido pelo Flamengo apÃ³s um empate sem gols.\n","\n","AlÃ©m do prestÃ­gio com o futebol, o Flamengo tambÃ©m se destaca em outras modalidades esportivas coletivas e individuais, notadamente no remo, no polo aquÃ¡tico e no basquetebol. Neste Ãºltimo, Ã© um dos clubes mais tradicionais do paÃ­s, tendo a sua equipe de basquetebol masculino conquistado quarenta e sete tÃ­tulos estaduais, oito tÃ­tulos Brasileiros, um Campeonato Sul-Americano de Clubes CampeÃµes, uma Liga Sul-Americana, uma Liga das AmÃ©ricas, uma Champions League AmÃ©ricas e duas Copas Intercontinentais FIBA.\n"]}]}},"499e477dcbe648d695d756feb7e16506":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":"1px solid black","bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DrXrwzfBh42s","executionInfo":{"status":"ok","timestamp":1762254161647,"user_tz":180,"elapsed":6389,"user":{"displayName":"Flavia Santos","userId":"00883513477943820333"}},"outputId":"7c104672-16c1-4f6d-934a-537d838efce6"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.5/1.6 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q transformers sentencepiece sentence-transformers ipywidgets nbformat"]},{"cell_type":"code","source":["!pip install -q transformers sentencepiece ipywidgets nbformat googletrans==4.0.0-rc1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FK1iiqVvsyxW","executionInfo":{"status":"ok","timestamp":1762256977961,"user_tz":180,"elapsed":17060,"user":{"displayName":"Flavia Santos","userId":"00883513477943820333"}},"outputId":"f9822571-993c-4b16-82bf-65b9763f841a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gradio 5.49.1 requires httpx<1.0,>=0.24.1, but you have httpx 0.13.3 which is incompatible.\n","mcp 1.19.0 requires httpx>=0.27.1, but you have httpx 0.13.3 which is incompatible.\n","openai 1.109.1 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n","google-genai 1.46.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\n","firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.13.3 which is incompatible.\n","langsmith 0.4.38 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n","gradio-client 1.13.3 requires httpx>=0.24.1, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["# Rode no Colab\n","!pip install -q transformers sentencepiece sentence-transformers ipywidgets nbformat\n"],"metadata":{"id":"T5v2KZXqtMqo","executionInfo":{"status":"ok","timestamp":1762257073109,"user_tz":180,"elapsed":7784,"user":{"displayName":"Flavia Santos","userId":"00883513477943820333"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# no Colab, apenas uma vez\n","!pip install -q wikipedia\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aeVDDClovaXe","executionInfo":{"status":"ok","timestamp":1762257662641,"user_tz":180,"elapsed":17415,"user":{"displayName":"Flavia Santos","userId":"00883513477943820333"}},"outputId":"9f78934b-048b-41a7-f2c9-04d08f171c17"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["# Arquivo gerado: colab_nlp_demo.py\n","# Este cÃ³digo deve ser executado em Google Colab (ou Jupyter). Ele cria uma UI simples com ipywidgets\n","# em portuguÃªs para escolher exemplos anexos, digitar texto, e aplicar tarefas NLP (geraÃ§Ã£o, resumo, traduÃ§Ã£o, Q&A).\n","# ObservaÃ§Ã£o: o cÃ³digo instala dependÃªncias ao rodar no Colab (comentÃ¡rios indicam onde executar pip).\n","\n","colab_code = r'''\n","# === Colab NLP Demo (PortuguÃªs) ===\n","# InstruÃ§Ãµes rÃ¡pidas:\n","# 1) Copie e cole todo este bloco em uma cÃ©lula no Colab (ou salve como .py e execute).\n","# 2) Execute a cÃ©lula; a primeira vez pode demorar (instala pacotes e baixa modelos).\n","# 3) Use a interface para escolher um exemplo e testar.\n","#\n","# Nota sobre traduÃ§Ã£o: o fluxo Ã© em portuguÃªs. Se um modelo sÃ³ aceitar inglÃªs, o texto serÃ¡ traduzido\n","# automaticamente para o inglÃªs, passado ao modelo, e o resultado serÃ¡ traduzido de volta para o portuguÃªs.\n","#\n","# RecomendaÃ§Ã£o de execuÃ§Ã£o em Colab: execute cada bloco em ordem. Se quiser trocar modelos, altere as strings de modelo abaixo.\n","\n","# --- (1) Instalar dependÃªncias (executar no Colab) ---\n","# !pip install -q transformers sentencepiece sentence-transformers ipywidgets nbformat\n","# Obs: se quiser traduÃ§Ã£o via googletrans em vez de MarianMT, descomente e instale:\n","# !pip install -q googletrans==4.0.0-rc1\n","\n","# --- (2) CÃ³digo principal ---\n","from IPython.display import display, HTML, clear_output\n","import ipywidgets as widgets\n","import os\n","import nbformat\n","from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n","from sentence_transformers import SentenceTransformer, util\n","\n","# --- ConfiguraÃ§Ãµes de modelo (ajuste se desejar) ---\n","# Modelos T5/Marian recomendados para traduÃ§Ã£o e seq2seq. Eles serÃ£o baixados quando executados no Colab.\n","MODEL_SUMMARY = \"facebook/bart-large-cnn\"        # resumir (funciona bem em inglÃªs)\n","MODEL_TRANSLATE_EN_PT = \"Helsinki-NLP/opus-mt-en-ROMANCE\"  # traduz EN -> ROMANCE (inclui pt)\n","MODEL_TRANSLATE_PT_EN = \"Helsinki-NLP/opus-mt-ROMANCE-en\"  # traduz ROMANCE -> EN (usar para pt->en)\n","MODEL_QA = \"deepset/roberta-base-squad2\"        # Q&A (inglÃªs)\n","MODEL_GEN = \"pierreguillou/t5-small-portuguese\" # geraÃ§Ã£o em portuguÃªs (seq2seq T5 fine-tuned em PT)\n","EMBEDDING_MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" # embeddings multilingue\n","\n","# --- FunÃ§Ãµes utilitÃ¡rias ---\n","def load_notebook_text(path):\n","    # Extrai texto de cÃ©lulas markdown e code de um .ipynb (se existir)\n","    try:\n","        nb = nbformat.read(path, as_version=4)\n","        texts = []\n","        for cell in nb.cells:\n","            if cell.cell_type in (\"markdown\", \"code\"):\n","                texts.append(cell.source)\n","        return \"\\n\\n\".join(texts)[:20000]  # limitar tamanho\n","    except Exception as e:\n","        return f\"(erro ao ler {path}: {e})\"\n","\n","# Carregar lista de exemplos no diretÃ³rio /mnt/data (Colab upload padrÃ£o)\n","EXAMPLES_DIR = \"/mnt/data\"\n","example_files = []\n","if os.path.exists(EXAMPLES_DIR):\n","    for f in os.listdir(EXAMPLES_DIR):\n","        if f.lower().endswith(\".ipynb\") or f.lower().endswith(\".txt\") or f.lower().endswith(\".md\"):\n","            example_files.append(f)\n","example_files = sorted(example_files)\n","\n","# --- Carregadores preguiÃ§osos para modelos (sÃ³ carregam quando necessÃ¡rio) ---\n","_loaded = {}\n","def get_translator_pt_en():\n","    if \"pt_en\" not in _loaded:\n","        _loaded[\"pt_en_tokenizer\"] = AutoTokenizer.from_pretrained(MODEL_TRANSLATE_PT_EN)\n","        _loaded[\"pt_en_model\"] = AutoModelForSeq2SeqLM.from_pretrained(MODEL_TRANSLATE_PT_EN)\n","    return _loaded[\"pt_en_tokenizer\"], _loaded[\"pt_en_model\"]\n","\n","def get_translator_en_pt():\n","    if \"en_pt\" not in _loaded:\n","        _loaded[\"en_pt_tokenizer\"] = AutoTokenizer.from_pretrained(MODEL_TRANSLATE_EN_PT)\n","        _loaded[\"en_pt_model\"] = AutoModelForSeq2SeqLM.from_pretrained(MODEL_TRANSLATE_EN_PT)\n","    return _loaded[\"en_pt_tokenizer\"], _loaded[\"en_pt_model\"]\n","\n","def translate_pt_to_en(text):\n","    tok, model = get_translator_pt_en()\n","    inputs = tok(text, return_tensors=\"pt\", truncation=True, max_length=512)\n","    outs = model.generate(**inputs, max_length=512)\n","    return tok.decode(outs[0], skip_special_tokens=True)\n","\n","def translate_en_to_pt(text):\n","    tok, model = get_translator_en_pt()\n","    inputs = tok(text, return_tensors=\"pt\", truncation=True, max_length=512)\n","    outs = model.generate(**inputs, max_length=512)\n","    return tok.decode(outs[0], skip_special_tokens=True)\n","\n","def ensure_english(text):\n","    # heurÃ­stica simples: se contÃ©m muitos caracteres acentuados e palavras PT comuns, traduzir\n","    pt_indicators = [\" que \", \" nÃ£o \", \" para \", \" por \", \" com \", \" Ã© \", \" estÃ¡ \", \" serÃ¡ \"]\n","    if any(w in text.lower() for w in pt_indicators):\n","        return translate_pt_to_en(text), True\n","    return text, False\n","\n","# --- Interface com ipywidgets ---\n","ex_select = widgets.Dropdown(\n","    options=[\"(Nenhum)\"] + example_files,\n","    description=\"Exemplo:\",\n","    layout=widgets.Layout(width=\"80%\")\n",")\n","\n","txt_area = widgets.Textarea(\n","    value=\"\",\n","    placeholder=\"Digite ou cole o texto aqui...\",\n","    description=\"Entrada:\",\n","    layout=widgets.Layout(width=\"98%\", height=\"160px\")\n",")\n","\n","task_select = widgets.Dropdown(\n","    options=[\"gerar_texto\", \"resumir\", \"traduzir_pt_en\", \"traduzir_en_pt\", \"pergunta_resposta\", \"semantic_search\"],\n","    value=\"gerar_texto\",\n","    description=\"Tarefa:\"\n",")\n","\n","model_select = widgets.Text(\n","    value=MODEL_GEN,\n","    description=\"Modelo:\",\n","    layout=widgets.Layout(width=\"80%\"),\n","    placeholder=\"Nome do modelo HF (opcional)\"\n",")\n","\n","btn = widgets.Button(description=\"Executar\", button_style=\"primary\")\n","\n","out = widgets.Output(layout={'border': '1px solid black'})\n","\n","# FunÃ§Ã£o principal ao clicar\n","def on_execute(b):\n","    with out:\n","        clear_output()\n","        # preparar texto de entrada\n","        chosen = ex_select.value\n","        if chosen and chosen != \"(Nenhum)\":\n","            path = os.path.join(EXAMPLES_DIR, chosen)\n","            base_text = load_notebook_text(path)\n","            print(f\"Usando exemplo: {chosen}\\n--- Trecho do exemplo ---\\n{base_text[:2000]}\\n--- Fim do trecho ---\\n\")\n","            # se o usuÃ¡rio digitou algo, concatena\n","            if txt_area.value.strip():\n","                base_text = base_text + \"\\n\\n\" + txt_area.value.strip()\n","        else:\n","            base_text = txt_area.value.strip()\n","        if not base_text:\n","            print(\"ERRO: nenhum texto de entrada fornecido.\")\n","            return\n","\n","        task = task_select.value\n","        chosen_model = model_select.value.strip() or None\n","\n","        # Gerar / Resumir / Traduzir / Q&A\n","        if task == \"gerar_texto\":\n","            # GeraÃ§Ã£o seq2seq: usa modelo T5-like se disponÃ­vel; roda em PT por padrÃ£o.\n","            model_name = chosen_model or MODEL_GEN\n","            print(f\"Gerando texto com {model_name} ... (pode demorar ao baixar modelo)\")\n","            gen_pipe = pipeline(\"text2text-generation\", model=model_name, tokenizer=model_name, device=0 if \"COLAB_GPU\" in os.environ else -1)\n","            prompt = f\"gerar: {base_text}\"\n","            res = gen_pipe(prompt, max_length=512, num_return_sequences=1)\n","            out_text = res[0][\"generated_text\"] if isinstance(res, list) else str(res)\n","            print(\"Resultado:\\n\", out_text)\n","\n","        elif task == \"resumir\":\n","            model_name = chosen_model or MODEL_SUMMARY\n","            # traduz para inglÃªs se necessÃ¡rio (bart-large-cnn Ã© em inglÃªs)\n","            txt_en, translated = ensure_english(base_text)\n","            if translated:\n","                print(\"(Texto traduzido para inglÃªs antes do resumo.)\")\n","            print(f\"Resumindo com {model_name} ...\")\n","            summarizer = pipeline(\"summarization\", model=model_name, tokenizer=model_name, device=0 if \"COLAB_GPU\" in os.environ else -1)\n","            summary = summarizer(txt_en, max_length=200, min_length=30, do_sample=False)\n","            summary_text = summary[0][\"summary_text\"]\n","            if translated:\n","                summary_text = translate_en_to_pt(summary_text)\n","            print(\"Resumo (pt):\\n\", summary_text)\n","\n","        elif task == \"traduzir_pt_en\":\n","            print(\"Traduzindo PT -> EN ...\")\n","            print(translate_pt_to_en(base_text))\n","\n","        elif task == \"traduzir_en_pt\":\n","            print(\"Traduzindo EN -> PT ...\")\n","            print(translate_en_to_pt(base_text))\n","\n","        elif task == \"pergunta_resposta\":\n","            # Q&A: se base_text Ã© contexto e usuÃ¡rio pergunta algo no final. Pedimos que o usuÃ¡rio escreva pergunta no final\n","            parts = base_text.split(\"\\n\")\n","            question = parts[-1] if len(parts)>0 else \"\"\n","            context = \"\\n\".join(parts[:-1]) if len(parts)>1 else base_text\n","            print(\"Contexto (trecho):\", context[:1000])\n","            # garantir inglÃªs para modelo padrÃ£o (deepset/roberta-base-squad2)\n","            txt_en_ctx, ctx_translated = ensure_english(context)\n","            q_en, q_translated = ensure_english(question)\n","            qa_model = chosen_model or MODEL_QA\n","            qa = pipeline(\"question-answering\", model=qa_model, tokenizer=qa_model, device=0 if \"COLAB_GPU\" in os.environ else -1)\n","            ans = qa(question=q_en, context=txt_en_ctx)\n","            answer = ans.get(\"answer\", \"\")\n","            if q_translated or ctx_translated:\n","                answer = translate_en_to_pt(answer)\n","            print(\"Resposta:\\n\", answer)\n","            print(\"\\nDetalhes do pipeline:\", ans)\n","\n","        elif task == \"semantic_search\":\n","            # Busca semÃ¢ntica simples: indexa input (pode ser mÃºltiplos documentos separados por '---doc---')\n","            print(\"Rodando busca semÃ¢ntica (paraphrase-multilingual-MiniLM-L12-v2)...\")\n","            embedder = SentenceTransformer(EMBEDDING_MODEL)\n","            docs = [d.strip() for d in base_text.split(\"---doc---\") if d.strip()]\n","            doc_emb = embedder.encode(docs, convert_to_tensor=True)\n","            query = widgets.Text(value=\"\", description=\"Consulta:\")\n","            btn2 = widgets.Button(description=\"Buscar\")\n","            out2 = widgets.Output()\n","            def on_search(b):\n","                with out2:\n","                    clear_output()\n","                    q = query.value.strip()\n","                    if not q:\n","                        print(\"Digite a consulta.\")\n","                        return\n","                    q_emb = embedder.encode(q, convert_to_tensor=True)\n","                    hits = util.semantic_search(q_emb, doc_emb, top_k=5)[0]\n","                    for h in hits:\n","                        idx = h[\"corpus_id\"]\n","                        score = h[\"score\"]\n","                        print(f\"--- Documento {idx} (score={score:.4f}) ---\")\n","                        print(docs[int(idx)][:1000])\n","                        print(\"\\n\")\n","            btn2.on_click(on_search)\n","            display(query, btn2, out2)\n","            print(\"Separe documentos com '---doc---' no texto de entrada. Use o campo acima para consultar.\")\n","\n","        else:\n","            print(\"Tarefa nÃ£o suportada.\")\n","\n","# ligar evento\n","btn.on_click(on_execute)\n","\n","# Mostrar interface\n","display(HTML('<h3>Demo NLP (PortuguÃªs) â€” Colab</h3>'))\n","display(ex_select)\n","display(txt_area)\n","display(task_select)\n","display(model_select)\n","display(btn)\n","display(out)\n","'''\n","\n","# Salvar arquivo no /mnt/data para o usuÃ¡rio baixar/usar\n","path = \"/content/colab_nlp_demo.py\"\n","with open(path, \"w\", encoding=\"utf-8\") as f:\n","    f.write(colab_code)\n","\n","# Listar arquivos no /mnt/data e mostrar o comeÃ§o do script\n","import os, textwrap\n","files = os.listdir(\"/content\")\n","preview = \"\"\n","with open(path, \"r\", encoding=\"utf-8\") as f:\n","    preview = \"\".join([next(f) for _ in range(60)])  # mostra primeiras 60 linhas\n","\n","print(\"Arquivos em /content:\")\n","for fi in files:\n","    print(\" -\", fi)\n","print(\"\\nArquivo criado:\", path)\n","print(\"\\nPreview do inÃ­cio do arquivo (60 linhas):\\n\")\n","print(preview)\n","print(\"\\nPara usar: abra o Colab, faÃ§a upload dos exemplos (se ainda nÃ£o estiverem em /mnt/data), copie todo o conteÃºdo do arquivo 'colab_nlp_demo.py' para uma cÃ©lula e execute. Siga as instruÃ§Ãµes no topo do arquivo para instalar dependÃªncias.\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0daUf1fEiJZu","executionInfo":{"status":"ok","timestamp":1762254307740,"user_tz":180,"elapsed":103,"user":{"displayName":"Flavia Santos","userId":"00883513477943820333"}},"outputId":"3f1c4c1e-020b-4d40-8be0-f0320de45814"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Arquivos em /content:\n"," - .config\n"," - colab_nlp_demo.py\n"," - sample_data\n","\n","Arquivo criado: /content/colab_nlp_demo.py\n","\n","Preview do inÃ­cio do arquivo (60 linhas):\n","\n","\n","# === Colab NLP Demo (PortuguÃªs) ===\n","# InstruÃ§Ãµes rÃ¡pidas:\n","# 1) Copie e cole todo este bloco em uma cÃ©lula no Colab (ou salve como .py e execute).\n","# 2) Execute a cÃ©lula; a primeira vez pode demorar (instala pacotes e baixa modelos).\n","# 3) Use a interface para escolher um exemplo e testar.\n","#\n","# Nota sobre traduÃ§Ã£o: o fluxo Ã© em portuguÃªs. Se um modelo sÃ³ aceitar inglÃªs, o texto serÃ¡ traduzido\n","# automaticamente para o inglÃªs, passado ao modelo, e o resultado serÃ¡ traduzido de volta para o portuguÃªs.\n","#\n","# RecomendaÃ§Ã£o de execuÃ§Ã£o em Colab: execute cada bloco em ordem. Se quiser trocar modelos, altere as strings de modelo abaixo.\n","\n","# --- (1) Instalar dependÃªncias (executar no Colab) ---\n","# !pip install -q transformers sentencepiece sentence-transformers ipywidgets nbformat\n","# Obs: se quiser traduÃ§Ã£o via googletrans em vez de MarianMT, descomente e instale:\n","# !pip install -q googletrans==4.0.0-rc1\n","\n","# --- (2) CÃ³digo principal ---\n","from IPython.display import display, HTML, clear_output\n","import ipywidgets as widgets\n","import os\n","import nbformat\n","from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n","from sentence_transformers import SentenceTransformer, util\n","\n","# --- ConfiguraÃ§Ãµes de modelo (ajuste se desejar) ---\n","# Modelos T5/Marian recomendados para traduÃ§Ã£o e seq2seq. Eles serÃ£o baixados quando executados no Colab.\n","MODEL_SUMMARY = \"facebook/bart-large-cnn\"        # resumir (funciona bem em inglÃªs)\n","MODEL_TRANSLATE_EN_PT = \"Helsinki-NLP/opus-mt-en-ROMANCE\"  # traduz EN -> ROMANCE (inclui pt)\n","MODEL_TRANSLATE_PT_EN = \"Helsinki-NLP/opus-mt-ROMANCE-en\"  # traduz ROMANCE -> EN (usar para pt->en)\n","MODEL_QA = \"deepset/roberta-base-squad2\"        # Q&A (inglÃªs)\n","MODEL_GEN = \"pierreguillou/t5-small-portuguese\" # geraÃ§Ã£o em portuguÃªs (seq2seq T5 fine-tuned em PT)\n","EMBEDDING_MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\" # embeddings multilingue\n","\n","# --- FunÃ§Ãµes utilitÃ¡rias ---\n","def load_notebook_text(path):\n","    # Extrai texto de cÃ©lulas markdown e code de um .ipynb (se existir)\n","    try:\n","        nb = nbformat.read(path, as_version=4)\n","        texts = []\n","        for cell in nb.cells:\n","            if cell.cell_type in (\"markdown\", \"code\"):\n","                texts.append(cell.source)\n","        return \"\\n\\n\".join(texts)[:20000]  # limitar tamanho\n","    except Exception as e:\n","        return f\"(erro ao ler {path}: {e})\"\n","\n","# Carregar lista de exemplos no diretÃ³rio /mnt/data (Colab upload padrÃ£o)\n","EXAMPLES_DIR = \"/mnt/data\"\n","example_files = []\n","if os.path.exists(EXAMPLES_DIR):\n","    for f in os.listdir(EXAMPLES_DIR):\n","        if f.lower().endswith(\".ipynb\") or f.lower().endswith(\".txt\") or f.lower().endswith(\".md\"):\n","            example_files.append(f)\n","example_files = sorted(example_files)\n","\n","# --- Carregadores preguiÃ§osos para modelos (sÃ³ carregam quando necessÃ¡rio) ---\n","_loaded = {}\n","def get_translator_pt_en():\n","    if \"pt_en\" not in _loaded:\n","\n","\n","Para usar: abra o Colab, faÃ§a upload dos exemplos (se ainda nÃ£o estiverem em /mnt/data), copie todo o conteÃºdo do arquivo 'colab_nlp_demo.py' para uma cÃ©lula e execute. Siga as instruÃ§Ãµes no topo do arquivo para instalar dependÃªncias.\n"]}]},{"cell_type":"code","source":["# CÃ“DIGO COMPLETO PARA COLOCAR NUMA CÃ‰LULA ÃšNICA DO COLAB\n","# Instalar antes se necessÃ¡rio:\n","# !pip install -q transformers sentencepiece sentence-transformers ipywidgets nbformat wikipedia\n","\n","from IPython.display import display, clear_output, HTML\n","import ipywidgets as widgets\n","import os, nbformat, torch, warnings, re, time\n","from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","# DiretÃ³rio com exemplos\n","EXAMPLES_DIR = \"/content\"\n","def list_examples():\n","    files = []\n","    if os.path.exists(EXAMPLES_DIR):\n","        for f in os.listdir(EXAMPLES_DIR):\n","            if f.lower().endswith((\".ipynb\", \".txt\", \".md\")):\n","                files.append(f)\n","    return sorted(files)\n","\n","def load_notebook_text(path):\n","    try:\n","        nb = nbformat.read(path, as_version=4)\n","        texts = []\n","        for cell in nb.cells:\n","            if cell.cell_type in (\"markdown\", \"code\"):\n","                texts.append(cell.source)\n","        return \"\\n\\n\".join(texts)[:20000]\n","    except Exception as e:\n","        return f\"(erro ao ler {path}: {e})\"\n","\n","# Modelos e configuraÃ§Ãµes\n","MODEL_PT_T5 = \"unicamp-dl/ptt5-base-portuguese-vocab\"  # geraÃ§Ã£o em PT preferida\n","MODEL_FLAN = \"google/flan-t5-base\"                     # fallback instruÃ­do\n","MODEL_NEO = \"EleutherAI/gpt-neo-125M\"                  # fallback causal\n","MODEL_MT5 = \"google/mt5-small\"                         # fallback multilingual (traduÃ§Ã£o/resumo)\n","MODEL_SUMMARY = \"facebook/bart-large-cnn\"              # summarizer EN\n","MARIA_PT_EN = \"Helsinki-NLP/opus-mt-pt-en\"\n","MARIA_EN_PT = \"Helsinki-NLP/opus-mt-en-pt\"\n","MODEL_QA = \"deepset/roberta-base-squad2\"\n","\n","device = 0 if torch.cuda.is_available() else -1\n","_loaded = {}\n","\n","# Tenta carregar tradutor Marian; se nÃ£o der, retorna (None, None)\n","def try_load_marien(name):\n","    key = f\"trans_{name}\"\n","    if key in _loaded:\n","        return _loaded[key]\n","    try:\n","        tok = AutoTokenizer.from_pretrained(name)\n","        model = AutoModelForSeq2SeqLM.from_pretrained(name)\n","        _loaded[key] = (tok, model)\n","        return tok, model\n","    except Exception:\n","        return None, None\n","\n","# TraduÃ§Ã£o: tenta Marian, se nÃ£o disponÃ­vel usa MT5 como fallback generator (prompt)\n","def translate_pt_to_en(text):\n","    tok, model = try_load_marien(MARIA_PT_EN)\n","    if tok is not None:\n","        inputs = tok(text, return_tensors=\"pt\", truncation=True, max_length=512)\n","        outs = model.generate(**inputs, max_length=512)\n","        return tok.decode(outs[0], skip_special_tokens=True)\n","    # fallback via MT5\n","    try:\n","        mtpipe = pipeline(\"text2text-generation\", model=MODEL_MT5, tokenizer=MODEL_MT5, device=device)\n","        out = mtpipe(f\"Translate to English: {text}\", max_new_tokens=256, do_sample=False, num_return_sequences=1)\n","        return (out[0].get(\"generated_text\") or out[0].get(\"text\") or str(out[0])).strip()\n","    except Exception:\n","        return text\n","\n","def translate_en_to_pt(text):\n","    tok, model = try_load_marien(MARIA_EN_PT)\n","    if tok is not None:\n","        inputs = tok(text, return_tensors=\"pt\", truncation=True, max_length=512)\n","        outs = model.generate(**inputs, max_length=512)\n","        return tok.decode(outs[0], skip_special_tokens=True)\n","    # fallback via MT5\n","    try:\n","        mtpipe = pipeline(\"text2text-generation\", model=MODEL_MT5, tokenizer=MODEL_MT5, device=device)\n","        out = mtpipe(f\"Translate to Portuguese: {text}\", max_new_tokens=256, do_sample=False, num_return_sequences=1)\n","        return (out[0].get(\"generated_text\") or out[0].get(\"text\") or str(out[0])).strip()\n","    except Exception:\n","        return text\n","\n","def ensure_english_if_possible(text):\n","    pt_indicators = [\" que \", \" nÃ£o \", \" para \", \" por \", \" com \", \" Ã© \", \" estÃ¡ \", \" serÃ¡ \", \" tambÃ©m \"]\n","    if any(w in text.lower() for w in pt_indicators):\n","        try:\n","            return translate_pt_to_en(text), True\n","        except Exception:\n","            return text, False\n","    return text, False\n","\n","# Helpers de geraÃ§Ã£o segura\n","def is_bad_output(text, prompt):\n","    t = (text or \"\").strip()\n","    if not t:\n","        return True\n","    if len(t) < 10:\n","        return True\n","    p_short = prompt.strip().lower()[:40]\n","    if t.lower().startswith(p_short):\n","        return True\n","    words = re.findall(r'\\w+', t.lower())\n","    if len(words) > 6 and len(set(words[-6:])) == 1:\n","        return True\n","    return False\n","\n","def safe_generate_text(pipe, prompt, max_new_tokens=150, deterministic=True):\n","    gen_kwargs = {\"max_new_tokens\": max_new_tokens, \"early_stopping\": True, \"no_repeat_ngram_size\": 3, \"repetition_penalty\": 1.2}\n","    if deterministic:\n","        gen_kwargs[\"do_sample\"] = False\n","    else:\n","        gen_kwargs.update({\"do_sample\": True, \"top_p\": 0.92, \"temperature\": 0.8})\n","    res = pipe(prompt, **gen_kwargs, num_return_sequences=1)\n","    if isinstance(res, list) and len(res) > 0:\n","        return res[0].get(\"generated_text\") or res[0].get(\"text\") or res[0].get(\"summary_text\") or str(res[0])\n","    return str(res)\n","\n","# UI widgets\n","ex_select = widgets.Dropdown(options=[\"(Nenhum)\"] + list_examples(), description=\"Exemplo:\", layout=widgets.Layout(width=\"80%\"))\n","txt_area = widgets.Textarea(value=\"\", placeholder=\"Digite ou cole o texto aqui (tema: esportes)\", description=\"Entrada:\", layout=widgets.Layout(width=\"98%\", height=\"160px\"))\n","task_select = widgets.Dropdown(options=[(\"Gerar texto\",\"gerar_texto\"),(\"Resumir\",\"resumir\"),(\"Traduzir PT->EN\",\"traduzir_pt_en\"),(\"Traduzir EN->PT\",\"traduzir_en_pt\"),(\"Pergunta/Resposta\",\"pergunta_resposta\")], value=\"gerar_texto\", description=\"Tarefa:\")\n","model_input = widgets.Text(value=\"\", description=\"Modelo (opcional):\", layout=widgets.Layout(width=\"80%\"))\n","run_btn = widgets.Button(description=\"Executar\", button_style=\"success\")\n","out = widgets.Output(layout={'border': '1px solid black'})\n","\n","# FunÃ§Ã£o principal\n","def on_run(b):\n","    with out:\n","        clear_output()\n","        chosen = ex_select.value\n","        if chosen and chosen != \"(Nenhum)\":\n","            path = os.path.join(EXAMPLES_DIR, chosen)\n","            base_text = load_notebook_text(path)\n","            if txt_area.value.strip():\n","                base_text = base_text + \"\\n\\n\" + txt_area.value.strip()\n","        else:\n","            base_text = txt_area.value.strip()\n","        if not base_text:\n","            print(\"ERRO: nenhum texto de entrada fornecido.\")\n","            return\n","\n","        task = task_select.value\n","        chosen_model = model_input.value.strip() or None\n","\n","        try:\n","            # ---------- GERAÃ‡ÃƒO (nova versÃ£o com Wikipedia como fonte principal) ----------\n","            if task == \"gerar_texto\":\n","                import wikipedia\n","                wikipedia.set_lang(\"pt\")\n","                tema = base_text.strip()\n","                if not tema:\n","                    print(\"ERRO: nenhum tema informado.\")\n","                else:\n","                    print(f\"ğŸ” Buscando resumo sobre '{tema}' na Wikipedia (PT)...\")\n","                    try:\n","                        results = wikipedia.search(tema, results=3)\n","                        if results:\n","                            page = wikipedia.page(results[0])\n","                            summary = page.summary\n","                            # limitar para evitar textos muito longos\n","                            paragraphs = summary.split(\"\\n\")\n","                            resumo_final = \"\\n\\n\".join(paragraphs[:5]).strip()\n","                            print(\"\\nğŸŸ¢ Resultado (Wikipedia):\\n\", resumo_final)\n","                        else:\n","                            print(\"âŒ Nenhum resultado encontrado na Wikipedia, tentando gerar texto com modelo...\")\n","\n","                            # fallback - geraÃ§Ã£o automÃ¡tica (somente se Wikipedia nÃ£o encontrou nada)\n","                            model_name = chosen_model or MODEL_FLAN\n","                            gen_pipe = pipeline(\"text2text-generation\", model=model_name, tokenizer=model_name, device=device)\n","                            prompt = (\n","                                f\"Escreva um texto informativo e coerente em portuguÃªs sobre o tema '{tema}'. \"\n","                                \"Use linguagem natural e ao menos 5 frases completas.\"\n","                            )\n","                            res = gen_pipe(prompt, max_new_tokens=220, do_sample=True, top_p=0.92, temperature=0.9)\n","                            texto = res[0].get(\"generated_text\") or res[0].get(\"text\") or str(res[0])\n","                            print(\"\\nğŸŸ¢ Resultado (gerado):\\n\", texto.strip())\n","                    except Exception as e:\n","                        print(\"Erro ao buscar na Wikipedia:\", e)\n","\n","            # ---------- RESUMO ----------\n","            elif task == \"resumir\":\n","                model_name = chosen_model or MODEL_SUMMARY\n","                txt_en, translated = ensure_english_if_possible(base_text)\n","                if translated:\n","                    print(\"(texto detectado em portuguÃªs â€” traduzindo para inglÃªs antes do resumo)\")\n","                try:\n","                    summarizer = pipeline(\"summarization\", model=model_name, tokenizer=model_name, device=device)\n","                    summary = summarizer(txt_en, max_length=200, min_length=30, do_sample=False)\n","                    summary_text = summary[0].get(\"summary_text\", \"\")\n","                    if translated and summary_text:\n","                        try:\n","                            summary_text = translate_en_to_pt(summary_text)\n","                        except Exception:\n","                            pass\n","                    print(\"\\nğŸŸ¢ Resumo:\\n\", summary_text)\n","                except Exception as e:\n","                    print(\"(Aviso) summarizer falhou:\", e)\n","                    # fallback via MT5\n","                    try:\n","                        mtpipe = pipeline(\"text2text-generation\", model=MODEL_MT5, tokenizer=MODEL_MT5, device=device)\n","                        prompt = f\"Resuma em portuguÃªs de forma clara: {base_text}\"\n","                        out_text = safe_generate_text(mtpipe, prompt, max_new_tokens=180, deterministic=True)\n","                        out_text = out_text.replace(\"<extra_id_0>\", \"\").strip()\n","                        print(\"\\nğŸŸ¢ Resumo (fallback):\\n\", out_text)\n","                    except Exception as e2:\n","                        print(\"Fallback de resumo falhou:\", e2)\n","\n","            # ---------- TRADUÃ‡ÃƒO PT->EN ----------\n","            elif task == \"traduzir_pt_en\":\n","                try:\n","                    res = translate_pt_to_en(base_text)\n","                    print(\"\\nğŸŸ¢ TraduÃ§Ã£o PT->EN:\\n\", res)\n","                except Exception as e:\n","                    print(\"âŒ Erro na traduÃ§Ã£o PT->EN:\", e)\n","                    # fallback mt5\n","                    try:\n","                        mtpipe = pipeline(\"text2text-generation\", model=MODEL_MT5, tokenizer=MODEL_MT5, device=device)\n","                        out_text = safe_generate_text(mtpipe, f\"Translate to English: {base_text}\", max_new_tokens=200, deterministic=True)\n","                        print(\"\\nğŸŸ¢ TraduÃ§Ã£o (fallback mt5):\\n\", out_text)\n","                    except Exception as e2:\n","                        print(\"Fallback de traduÃ§Ã£o falhou:\", e2)\n","\n","            # ---------- TRADUÃ‡ÃƒO EN->PT ----------\n","            elif task == \"traduzir_en_pt\":\n","                try:\n","                    res = translate_en_to_pt(base_text)\n","                    print(\"\\nğŸŸ¢ TraduÃ§Ã£o EN->PT:\\n\", res)\n","                except Exception as e:\n","                    print(\"âŒ Erro na traduÃ§Ã£o EN->PT:\", e)\n","                    try:\n","                        mtpipe = pipeline(\"text2text-generation\", model=MODEL_MT5, tokenizer=MODEL_MT5, device=device)\n","                        out_text = safe_generate_text(mtpipe, f\"Translate to Portuguese: {base_text}\", max_new_tokens=200, deterministic=True)\n","                        print(\"\\nğŸŸ¢ TraduÃ§Ã£o (fallback mt5):\\n\", out_text)\n","                    except Exception as e2:\n","                        print(\"Fallback de traduÃ§Ã£o falhou:\", e2)\n","\n","            # ---------- PERGUNTA/RESPOSTA ----------\n","            elif task == \"pergunta_resposta\":\n","                parts = base_text.strip().split(\"\\n\")\n","                if len(parts) > 1:\n","                    question = parts[-1].strip()\n","                    context = \"\\n\".join(parts[:-1]).strip()\n","                else:\n","                    question = parts[0].strip()\n","                    context = \"\"\n","\n","                if context:\n","                    print(\"Respondendo com base no contexto (extractive QA)...\")\n","                    try:\n","                        ctx_en, ctx_translated = ensure_english_if_possible(context)\n","                    except Exception:\n","                        ctx_en, ctx_translated = context, False\n","                    try:\n","                        q_en, q_translated = ensure_english_if_possible(question)\n","                    except Exception:\n","                        q_en, q_translated = question, False\n","                    qa = pipeline(\"question-answering\", model=chosen_model or MODEL_QA, tokenizer=chosen_model or MODEL_QA, device=device)\n","                    ans = qa(question=q_en, context=ctx_en)\n","                    answer = ans.get(\"answer\",\"\")\n","                    if (ctx_translated or q_translated) and answer:\n","                        try:\n","                            answer = translate_en_to_pt(answer)\n","                        except Exception:\n","                            pass\n","                    print(\"\\nğŸŸ¢ Resposta:\", answer)\n","                    print(\"\\nDetalhes:\", ans)\n","                else:\n","                    print(\"Sem contexto â€” tentando resposta pela Wikipedia (consulta rÃ¡pida)...\")\n","                    try:\n","                        import wikipedia\n","                        wikipedia.set_lang(\"pt\")\n","                        hits = wikipedia.search(question, results=5)\n","                        if not hits:\n","                            raise Exception(\"Nenhum resultado na Wikipedia\")\n","                        page_title = hits[0]\n","                        summary = wikipedia.summary(page_title, sentences=3)\n","                        print(\"\\nğŸŸ¢ Resposta (via Wikipedia - verifique):\\n\", summary)\n","                    except Exception as e:\n","                        print(\"(Wikipedia falhou ou sem resultado):\", e)\n","                        print(\"Fallback: gerando resposta com modelo (pode nÃ£o ser 100% confiÃ¡vel).\")\n","                        try:\n","                            gen = pipeline(\"text2text-generation\", model=MODEL_FLAN, tokenizer=MODEL_FLAN, device=device)\n","                            prompt = f\"Instruction: Responda de forma objetiva e curta.\\nInput: {question}\\nOutput:\"\n","                            ans = safe_generate_text(gen, prompt, max_new_tokens=120, deterministic=False)\n","                            ans = ans.replace(\"<extra_id_0>\", \"\").strip()\n","                            print(\"\\nğŸŸ¢ Resposta (gerada):\\n\", ans)\n","                        except Exception as e2:\n","                            print(\"Fallback gerador falhou:\", e2)\n","\n","            else:\n","                print(\"Tarefa nÃ£o suportada.\")\n","        except Exception as e:\n","            print(\"Erro durante a execuÃ§Ã£o:\", e)\n","\n","# ligar evento\n","run_btn.on_click(on_run)\n","\n","# Mostrar UI\n","display(HTML(\"<h3>Demo NLP â€” VersÃ£o Final (Colab) â€” DomÃ­nio: Esportes</h3><p>Foco: GeraÃ§Ã£o em portuguÃªs, resumo, traduÃ§Ã£o e QA. Recomendo ativar GPU (Runtime â†’ Change runtime type â†’ GPU).</p>\"))\n","display(ex_select)\n","display(txt_area)\n","display(task_select)\n","display(model_input)\n","display(run_btn)\n","display(out)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":751,"referenced_widgets":["fd7b5b2f25d547f38ca1391c4fc24265","20ef53dc2a194f08aab8debe9ba02379","d5ccd487d53c42f992d357ff507a5955","85ec04e5f05a453f9c367758068eabfd","557505e3354e4542844331e640915264","01c3ef97dcc740348c86b9820cf00737","042c07c99e3c4c2ea72dcfb0619a9ec8","37ae4d0a411047aeafa9826245100f20","e6faef02f4494d1c895b9451d2fe3a2f","7e0bd5d3997e4ea6aa12f960f0db17fa","6af8a25a9e8d42aca14c2d4f17753fef","d4cce9a577024e10b57c5e61294b1d3d","3a0327611feb4d38865bbd181a229d1e","136f39e9b4b045f7b5fd3089ac01c9a0","9f2106118ea44adaa24a87dd0934cb57","772f3b6200a1426297da0ca6efe5d664","499e477dcbe648d695d756feb7e16506"]},"id":"C2i2uRU4121n","executionInfo":{"status":"ok","timestamp":1762260905654,"user_tz":180,"elapsed":138,"user":{"displayName":"Flavia Santos","userId":"00883513477943820333"}},"outputId":"9bb891b4-0a65-4997-8037-b4e1f6ea57b5"},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<h3>Demo NLP â€” VersÃ£o Final (Colab) â€” DomÃ­nio: Esportes</h3><p>Foco: GeraÃ§Ã£o em portuguÃªs, resumo, traduÃ§Ã£o e QA. Recomendo ativar GPU (Runtime â†’ Change runtime type â†’ GPU).</p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Dropdown(description='Exemplo:', layout=Layout(width='80%'), options=('(Nenhum)', 'raw_debug.txt'), value='(Neâ€¦"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd7b5b2f25d547f38ca1391c4fc24265"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Textarea(value='', description='Entrada:', layout=Layout(height='160px', width='98%'), placeholder='Digite ou â€¦"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85ec04e5f05a453f9c367758068eabfd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Dropdown(description='Tarefa:', options=(('Gerar texto', 'gerar_texto'), ('Resumir', 'resumir'), ('Traduzir PTâ€¦"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"042c07c99e3c4c2ea72dcfb0619a9ec8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Text(value='', description='Modelo (opcional):', layout=Layout(width='80%'))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e0bd5d3997e4ea6aa12f960f0db17fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Button(button_style='success', description='Executar', style=ButtonStyle())"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a0327611feb4d38865bbd181a229d1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Output(layout=Layout(border='1px solid black'))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"772f3b6200a1426297da0ca6efe5d664"}},"metadata":{}}]}]}